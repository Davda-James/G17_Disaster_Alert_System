% ============================================================================
% Disaster Alert System (DAS) - Test Documentation Report
% Reference: IEEE 829 Test Documentation Standard
% Reference: ISO/IEC/IEEE 29119 Software Testing
% ============================================================================

\documentclass[12pt,a4paper]{article}

% ============================================================================
% PACKAGES
% ============================================================================
\usepackage[margin=1in]{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage[style=ieee,backend=biber]{biblatex}
\usepackage{tocloft}

% ============================================================================
% CONFIGURATION
% ============================================================================

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    pdftitle={DAS Test Documentation},
    pdfauthor={DAS Development Team},
}

% Code listing style
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    language=Python,
}

\lstset{style=pythonstyle}

% Headers and footers
\pagestyle{fancy}
\fancyhf{}
\rhead{DAS Test Documentation}
\lhead{Assignment 1 - Group 17}
\rfoot{Page \thepage}

% ============================================================================
% DOCUMENT
% ============================================================================

\begin{document}

% ----------------------------------------------------------------------------
% TITLE PAGE
% ----------------------------------------------------------------------------
\begin{titlepage}
    \centering
    \vspace*{2cm}
    
    {\Huge\bfseries Disaster Alert System (DAS)\\[0.5cm]}
    {\LARGE Test Documentation Report\\[2cm]}
    
    \vfill
    
    {\Large\bfseries Assignment 1: Collaborative Software Development\\[0.5cm]}
    {\large Group 17\\[2cm]}
    
    \begin{tabular}{ll}
        \textbf{Course:} & Software Engineering (Semester 6) \\
        \textbf{Date:} & \today \\
        \textbf{Version:} & 1.0.0 \\
    \end{tabular}
    
    \vfill
    
    {\small Reference Standards: IEEE 829, ISO/IEC/IEEE 29119, ISTQB}
    
\end{titlepage}

% ----------------------------------------------------------------------------
% TABLE OF CONTENTS
% ----------------------------------------------------------------------------
\tableofcontents
\newpage

% ----------------------------------------------------------------------------
% EXECUTIVE SUMMARY
% ----------------------------------------------------------------------------
\section{Executive Summary}

This document presents a comprehensive test documentation for the \textbf{Disaster Alert System (DAS)}, a mission-critical platform designed to detect and disseminate alerts for natural disasters including earthquakes, tsunamis, floods, and cyclones.

\subsection{Project Objectives}
\begin{itemize}
    \item Develop a robust testing framework covering functional, integration, stress, and boundary value analysis
    \item Implement Risk-Based Testing (RBT) for safety-critical failure modes
    \item Create an automated evaluation framework with industry-standard metrics
    \item Ensure compliance with IEEE 829 and ISO/IEC/IEEE 29119 standards
\end{itemize}

\subsection{Key Metrics Summary}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Target} & \textbf{Actual Result} & \textbf{Status} \\
\midrule
Total Test Cases & 50+ & \textbf{79} & \textcolor{green}{$\checkmark$} \\
Test Success Rate & $\geq 95\%$ & \textbf{100.0\%} & \textcolor{green}{$\checkmark$} \\
Code Coverage & $\geq 80\%$ & \textbf{89.76\%} & \textcolor{green}{$\checkmark$} \\
Defect Density & $\leq 0.05$ & \textbf{0.0} & \textcolor{green}{$\checkmark$} \\
Execution Time & -- & \textbf{1.75s} & -- \\
\bottomrule
\end{tabular}
\caption{Evaluation Metrics Summary - \textbf{Status: PROD\_READY}}
\end{table}

\subsection{Verification vs. Validation}

\begin{description}
    \item[Verification:] ``Did we build the system right?'' \\
    Our test suite verifies that the DAS implementation correctly follows the specification requirements, including alert thresholds, severity classification, and notification delivery.
    
    \item[Validation:] ``Did we build the right system?'' \\
    Risk-Based Testing (RBT) validates that the system will function correctly during actual disaster scenarios, including network failures, database corruption, and high-load conditions.
\end{description}

% ----------------------------------------------------------------------------
% TEST METHODOLOGY
% ----------------------------------------------------------------------------
\section{Test Methodology}
\label{sec:methodology}

\subsection{Reference Standards}

Our testing methodology is based on internationally recognized standards:

\begin{description}
    \item[IEEE 829-2008] Standard for Software and System Test Documentation. Defines the format for test plans, test cases, and test reports.
    
    \item[ISO/IEC/IEEE 29119] Software and Systems Engineering --- Software Testing. Provides a comprehensive framework for test processes, techniques, and documentation.
    
    \item[ISTQB Foundation Level] International Software Testing Qualifications Board. Establishes best practices for test design techniques including boundary value analysis.
\end{description}

\subsection{Test Design Techniques}

\subsubsection{Equivalence Partitioning}
Input domain is divided into equivalence classes:
\begin{itemize}
    \item Valid inputs above threshold (trigger alert)
    \item Valid inputs below threshold (no alert)
    \item Invalid inputs (negative values, empty locations)
\end{itemize}

\subsubsection{Boundary Value Analysis}
Testing at exact boundary points for each threshold:
\begin{align}
    \text{Test Points} &= \{T - \epsilon, T, T + \epsilon\}
\end{align}
Where $T$ is the threshold and $\epsilon$ is a small delta (typically 0.01).

\subsubsection{Risk-Based Testing}
For mission-critical systems, tests are prioritized by risk:
\begin{equation}
    \text{Risk Priority} = \text{Likelihood} \times \text{Impact}
\end{equation}

\subsection{Test Categories}

\begin{table}[h]
\centering
\begin{tabular}{lllc}
\toprule
\textbf{Category} & \textbf{Purpose} & \textbf{Test IDs} & \textbf{Count} \\
\midrule
Functional & Core logic verification & FT-001 to FT-013 & 18 \\
Boundary & Edge case validation & BVA-001 to BVA-015 & 40 \\
Integration & End-to-end flows & IT-001 to IT-006 & 6 \\
Stress & Performance under load & ST-001 to ST-006 & 6 \\
Safety & Failure mode handling & RBT-001 to RBT-009 & 9 \\
\midrule
\textbf{Total} & & & \textbf{79} \\
\bottomrule
\end{tabular}
\caption{Test Categories Overview}
\end{table}

% ----------------------------------------------------------------------------
% RISK MATRIX
% ----------------------------------------------------------------------------
\section{Risk Matrix}
\label{sec:risk}

\subsection{Risk Assessment Methodology}

Risk levels are categorized according to ISO 22324 (Societal Security):

\begin{table}[h]
\centering
\begin{tabular}{ccccc}
\toprule
& \multicolumn{4}{c}{\textbf{Impact}} \\
\textbf{Likelihood} & Minor & Major & Critical & Catastrophic \\
\midrule
High & Medium & High & Critical & Critical \\
Medium & Low & Medium & High & Critical \\
Low & Low & Low & Medium & High \\
\bottomrule
\end{tabular}
\caption{Risk Priority Matrix}
\end{table}

\subsection{Identified Risks}

\begin{longtable}{p{2cm}p{4cm}p{2cm}p{2cm}p{3cm}}
\toprule
\textbf{Risk ID} & \textbf{Description} & \textbf{Impact} & \textbf{Priority} & \textbf{Mitigation} \\
\midrule
\endhead

RISK-001 & SMS gateway fails during Tsunami alert & Catastrophic & Critical & Retry logic, email fallback \\
RISK-002 & Database corruption during ongoing disaster & Critical & Critical & Cache fallback mechanism \\
RISK-003 & Network latency delays notifications & Major & High & Timeout handling, async queues \\
RISK-004 & Burst of 100+ simultaneous alerts & Major & High & Concurrent processing, thread safety \\
RISK-005 & Invalid sensor data triggers false alert & Major & Medium & Input validation, threshold review \\
\bottomrule
\caption{Risk Register}
\end{longtable}

% ----------------------------------------------------------------------------
% REQUIREMENTS TRACEABILITY MATRIX
% ----------------------------------------------------------------------------
\section{Requirements Traceability Matrix}
\label{sec:rtm}

The Requirements Traceability Matrix (RTM) maps system requirements to test cases, ensuring complete coverage and enabling impact analysis.

\subsection{Functional Requirements Traceability}

\begin{longtable}{p{1.5cm}p{4.5cm}p{3cm}p{1.5cm}p{1.5cm}}
\toprule
\textbf{Req ID} & \textbf{Requirement} & \textbf{Test Cases} & \textbf{Status} & \textbf{Coverage} \\
\midrule
\endhead

FR-001 & System shall generate alerts when earthquake magnitude $\geq$ 5.0 & FT-001, FT-002, BVA-001 to BVA-007 & PASS & 100\% \\
FR-002 & System shall generate alerts when tsunami wave height $\geq$ 2.0m & FT-003, BVA-008 & PASS & 100\% \\
FR-003 & System shall generate alerts when flood level $\geq$ 3.0m & FT-004, BVA-009 & PASS & 100\% \\
FR-004 & System shall generate alerts when cyclone wind speed $\geq$ 120 km/h & FT-005, BVA-010 & PASS & 100\% \\
FR-005 & System shall classify severity into 5 levels (LOW to CATASTROPHIC) & FT-006, FT-007, FT-008 & PASS & 100\% \\
FR-006 & System shall support alert acknowledgment & FT-009, FT-010, IT-004 & PASS & 100\% \\
FR-007 & System shall invoke callbacks on alert generation & FT-011, FT-012 & PASS & 100\% \\
FR-008 & System shall provide alert statistics & FT-013 & PASS & 100\% \\
FR-009 & System shall send SMS notifications & IT-001, IT-002, ST-003 & PASS & 100\% \\
FR-010 & System shall send email notifications & IT-002, ST-003 & PASS & 100\% \\
FR-011 & System shall store alerts in database & IT-001, IT-003 & PASS & 100\% \\
FR-012 & System shall filter contacts by region & IT-005 & PASS & 100\% \\
FR-013 & System shall order contacts by priority & IT-006 & PASS & 100\% \\
\bottomrule
\caption{Functional Requirements Traceability}
\end{longtable}

\subsection{Non-Functional Requirements Traceability}

\begin{longtable}{p{1.5cm}p{4.5cm}p{3cm}p{1.5cm}p{1.5cm}}
\toprule
\textbf{Req ID} & \textbf{Requirement} & \textbf{Test Cases} & \textbf{Status} & \textbf{Coverage} \\
\midrule
\endhead

NFR-001 & System shall process 100 alerts in $<$ 2 seconds & ST-001 & PASS & 100\% \\
NFR-002 & System shall support concurrent processing & ST-002, ST-005 & PASS & 100\% \\
NFR-003 & System shall deliver 100 notifications in $<$ 5 seconds & ST-003 & PASS & 100\% \\
NFR-004 & System shall maintain memory stability under load & ST-006 & PASS & 100\% \\
NFR-005 & System shall handle gateway failures gracefully & RBT-001, RBT-002, RBT-003 & PASS & 100\% \\
NFR-006 & System shall fall back to cache during DB corruption & RBT-004, RBT-005 & PASS & 100\% \\
NFR-007 & System shall continue operating under high latency & RBT-006 & PASS & 100\% \\
NFR-008 & System shall preserve alert data during failures & RBT-007, RBT-008, RBT-009 & PASS & 100\% \\
NFR-009 & System shall validate input boundaries correctly & BVA-011 to BVA-015 & PASS & 100\% \\
\bottomrule
\caption{Non-Functional Requirements Traceability}
\end{longtable}

\subsection{Risk-to-Test Mapping Matrix}

\begin{longtable}{p{2cm}p{4.5cm}p{3cm}p{2cm}p{1.5cm}}
\toprule
\textbf{Risk ID} & \textbf{Risk Description} & \textbf{Test Cases} & \textbf{Mitigation} & \textbf{Verified} \\
\midrule
\endhead

RISK-001 & SMS gateway fails during Tsunami alert & RBT-001, RBT-003 & Retry logic, email fallback & \checkmark PASS \\
RISK-002 & Database corruption during ongoing disaster & RBT-004, RBT-005 & Cache fallback mechanism & \checkmark PASS \\
RISK-003 & Network latency delays notifications & RBT-006 & Timeout handling & \checkmark PASS \\
RISK-004 & Burst of 100+ simultaneous alerts & ST-001, ST-002 & Concurrent processing & \checkmark PASS \\
RISK-005 & Invalid sensor data triggers false alert & BVA-005, BVA-011, BVA-012 & Input validation & \checkmark PASS \\
\bottomrule
\caption{Risk Mitigation Verification Matrix}
\end{longtable}

\subsection{Test Coverage Summary Matrix}

\begin{table}[h]
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Component} & \textbf{Func.} & \textbf{BVA} & \textbf{Integ.} & \textbf{Stress} & \textbf{Safety} \\
\midrule
Alert Manager & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
SMS Gateway & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
Email Gateway & -- & \checkmark & \checkmark & \checkmark & \checkmark \\
Database Manager & \checkmark & -- & \checkmark & \checkmark & \checkmark \\
Notification Service & \checkmark & -- & \checkmark & \checkmark & \checkmark \\
Config Module & -- & -- & -- & -- & \checkmark \\
Logger & -- & -- & -- & -- & \checkmark \\
\midrule
\textbf{Coverage} & 18 & 40 & 6 & 6 & 9 \\
\bottomrule
\end{tabular}
\caption{Component Test Coverage Matrix}
\end{table}

\subsection{Traceability Summary}

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Category} & \textbf{Requirements} & \textbf{Tests Mapped} & \textbf{Passed} & \textbf{Coverage} \\
\midrule
Functional Requirements & 13 & 45 & 45 & 100\% \\
Non-Functional Requirements & 9 & 30 & 30 & 100\% \\
Risk Mitigations & 5 & 12 & 12 & 100\% \\
\midrule
\textbf{Total} & \textbf{27} & \textbf{79*} & \textbf{79} & \textbf{100\%} \\
\bottomrule
\end{tabular}
\caption{Overall Traceability Summary (*some tests cover multiple requirements)}
\end{table}

% ----------------------------------------------------------------------------
% TEST CASE TABLES
% ----------------------------------------------------------------------------
\section{Detailed Test Cases}
\label{sec:testcases}

\subsection{Functional Tests}

\begin{longtable}{p{1.5cm}p{1cm}p{4cm}p{3cm}p{3.5cm}}
\toprule
\textbf{Test ID} & \textbf{Pri.} & \textbf{Description} & \textbf{Input} & \textbf{Expected Result} \\
\midrule
\endhead

FT-001 & P1 & Earthquake alert triggers above threshold & Magnitude: 6.5, Location: Tokyo & Alert generated, Severity: HIGH+ \\
FT-002 & P1 & No alert below threshold & Magnitude: 4.5 & No alert generated \\
FT-003 & P1 & Tsunami critical severity & Wave height: 12.0m & Severity: CRITICAL/CATASTROPHIC \\
FT-004 & P2 & Flood alert generation & Water level: 5.5m & Alert with severity HIGH \\
FT-005 & P2 & Cyclone alert generation & Wind: 200 km/h & Alert with severity HIGH \\
FT-006 & P2 & Severity LOW classification & Magnitude: 3.5 & Severity: LOW \\
FT-007 & P1 & Severity CATASTROPHIC classification & Magnitude: 9.0 & Severity: CATASTROPHIC \\
FT-008 & P2 & All earthquake severity levels & Various magnitudes & Correct classification \\
FT-009 & P2 & Acknowledge alert & Valid alert ID & acknowledged = True \\
FT-010 & P3 & Acknowledge non-existent & Invalid ID & Returns False \\
FT-011 & P1 & Callback invoked on alert & Registered callback & Callback called \\
FT-012 & P2 & Multiple callbacks & 2 callbacks & Both invoked \\
FT-013 & P3 & Statistics calculation & 3 alerts & Accurate counts \\
\bottomrule
\caption{Functional Test Cases}
\end{longtable}

\subsection{Boundary Value Analysis Tests}

\begin{longtable}{p{1.5cm}p{1cm}p{4cm}p{2.5cm}p{3.5cm}}
\toprule
\textbf{Test ID} & \textbf{Pri.} & \textbf{Description} & \textbf{Input} & \textbf{Expected Result} \\
\midrule
\endhead

BVA-001 & P1 & Earthquake just below threshold & 4.99 & No alert \\
BVA-002 & P1 & Earthquake exactly at threshold & 5.0 & Alert triggered \\
BVA-003 & P1 & Earthquake just above threshold & 5.01 & Alert triggered \\
BVA-004 & P2 & Earthquake zero value & 0 & No alert (valid input) \\
BVA-005 & P1 & Negative magnitude (invalid) & -1.0 & No alert, graceful handling \\
BVA-006 & P2 & Maximum Richter value & 10.0 & CATASTROPHIC severity \\
BVA-007 & P3 & Beyond maximum (edge case) & 12.0 & System handles gracefully \\
BVA-008 & P1 & Tsunami threshold boundaries & 1.99, 2.0, 2.01 & Correct trigger behavior \\
BVA-009 & P1 & Flood threshold boundaries & 2.99, 3.0, 3.01 & Correct trigger behavior \\
BVA-010 & P1 & Cyclone threshold boundaries & 119.9, 120.0, 120.1 & Correct trigger behavior \\
BVA-011 & P1 & Empty location string & "" & No alert (invalid input) \\
BVA-012 & P2 & Whitespace-only location & "   " & No alert \\
BVA-013 & P3 & Location with extra spaces & "  Tokyo  " & Alert with trimmed location \\
BVA-014 & P2 & Phone number length boundaries & Various lengths & Correct validation \\
BVA-015 & P2 & Email format boundaries & Various formats & Correct validation \\
\bottomrule
\caption{Boundary Value Analysis Test Cases}
\end{longtable}

\subsection{Risk-Based Safety Tests}

\begin{longtable}{p{1.5cm}p{1cm}p{4cm}p{2.5cm}p{3.5cm}}
\toprule
\textbf{Test ID} & \textbf{Risk} & \textbf{Failure Scenario} & \textbf{Precondition} & \textbf{Expected Behavior} \\
\midrule
\endhead

RBT-001 & CATA & SMS gateway fails during alert & Network failure enabled & SERVICE\_UNAVAILABLE, no crash \\
RBT-002 & CRIT & Email gateway fails & Network failure enabled & Graceful failure handling \\
RBT-003 & CATA & Notification retry exhausted & All retries fail & System continues operating \\
RBT-004 & CRIT & Database corruption fallback & DB corruption enabled & Falls back to cache \\
RBT-005 & CATA & No cache during corruption & No cache file & Graceful handling \\
RBT-006 & MAJOR & High latency impact & 100ms latency & Slower but successful \\
RBT-007 & CATA & Alert manager isolation & Gateway down & Alerts still recorded \\
RBT-008 & CATA & Multi-component recovery & Failure then recovery & System recovers \\
RBT-009 & CRIT & Alert data preservation & Notification fails & Alert record preserved \\
\bottomrule
\caption{Risk-Based Safety Test Cases}
\end{longtable}

\subsection{Integration Tests}

\begin{longtable}{p{1.5cm}p{1cm}p{4.5cm}p{6cm}}
\toprule
\textbf{Test ID} & \textbf{Pri.} & \textbf{Description} & \textbf{Expected Result} \\
\midrule
\endhead

IT-001 & P1 & Complete alert flow: sensor to SMS & Alert created, SMS sent, record stored in database \\
IT-002 & P1 & Multi-channel notification & All contacts receive both SMS and email \\
IT-003 & P2 & Database to AlertManager link & Alerts stored and retrievable by location \\
IT-004 & P2 & Alert acknowledgment flow & Acknowledgment persists, operator tracked \\
IT-005 & P1 & Region-based alert distribution & Only contacts in affected region notified \\
IT-006 & P2 & Contact priority ordering & Contacts returned sorted by priority level \\
\bottomrule
\caption{Integration Test Cases}
\end{longtable}

\subsection{Stress Tests}

\begin{longtable}{p{1.5cm}p{1cm}p{4cm}p{2.5cm}p{3.5cm}}
\toprule
\textbf{Test ID} & \textbf{Pri.} & \textbf{Description} & \textbf{Load} & \textbf{Performance Target} \\
\midrule
\endhead

ST-001 & P1 & Burst alert processing & 100 alerts & $<$ 2 seconds total \\
ST-002 & P1 & Concurrent processing & 5 threads, 50 alerts & $<$ 3 seconds \\
ST-003 & P1 & Notification throughput & 50 SMS + 50 email & $<$ 5 seconds \\
ST-004 & P2 & Bulk contact insertion & 100 contacts & $<$ 1 second \\
ST-005 & P2 & Concurrent DB queries & 50 queries, 5 threads & $<$ 2 seconds \\
ST-006 & P2 & Extended operation & 200 alerts, cleanup & Memory stable \\
\bottomrule
\caption{Stress Test Cases}
\end{longtable}

% ----------------------------------------------------------------------------
% CODE LISTINGS
% ----------------------------------------------------------------------------
\section{Code Listings}
\label{sec:code}

\subsection{Alert Manager - Severity Classification}

\begin{lstlisting}[caption={Severity Level Classification Logic}]
class SeverityLevel(Enum):
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4
    CATASTROPHIC = 5  # Reserved for life-threatening scenarios
    
    @classmethod
    def from_value(cls, value: float, disaster_type: DisasterType) -> 'SeverityLevel':
        """
        Determine severity based on sensor value and disaster type.
        Uses Risk-Based Testing (RBT) principles.
        """
        thresholds = {
            DisasterType.EARTHQUAKE: [
                (3.0, cls.LOW), (5.0, cls.MEDIUM), 
                (6.5, cls.HIGH), (7.5, cls.CRITICAL), 
                (8.5, cls.CATASTROPHIC)
            ],
            DisasterType.TSUNAMI: [
                (0.5, cls.LOW), (2.0, cls.MEDIUM), 
                (5.0, cls.HIGH), (10.0, cls.CRITICAL), 
                (15.0, cls.CATASTROPHIC)
            ],
        }
        
        type_thresholds = thresholds.get(disaster_type, [])
        for threshold, severity in reversed(type_thresholds):
            if value >= threshold:
                return severity
        return cls.LOW
\end{lstlisting}

\subsection{Evaluator - Metrics Calculation}

\begin{lstlisting}[caption={Test Metrics Calculation}]
def calculate_metrics(self, results: List[TestResult], coverage: float) -> EvaluationMetrics:
    """Calculate all evaluation metrics."""
    total = len(results)
    passed = sum(1 for r in results if r.status == "passed")
    failed = sum(1 for r in results if r.status == "failed")
    skipped = sum(1 for r in results if r.status == "skipped")
    errors = sum(1 for r in results if r.status == "error")
    
    # Success rate (excluding skipped)
    executed = total - skipped
    success_rate = (passed / executed * 100) if executed > 0 else 0.0
    
    # Defect density = failed tests / total tests
    defect_density = (failed + errors) / total if total > 0 else 0.0
    
    # Determine status
    status = self._determine_status(success_rate, coverage, defect_density)
    
    return EvaluationMetrics(
        total_tests=total,
        passed=passed,
        failed=failed,
        success_rate=round(success_rate, 2),
        code_coverage=round(coverage, 2),
        defect_density=round(defect_density, 4),
        status=status
    )
\end{lstlisting}

\subsection{Test Example - Boundary Value Analysis}

\begin{lstlisting}[caption={Boundary Value Test for Earthquake Threshold}]
@pytest.mark.parametrize("magnitude,should_alert", [
    (4.99, False),  # Just below threshold
    (5.0, True),    # Exactly at threshold
    (5.01, True),   # Just above threshold
    (0.0, False),   # Minimum valid
    (-1.0, False),  # Invalid negative
    (12.0, True),   # Extreme value
])
def test_bva008_earthquake_boundaries(self, alert_manager, magnitude, should_alert):
    """
    Test ID: BVA-008
    Tests all boundary points for earthquake threshold (5.0)
    """
    alert = alert_manager.process_sensor_data(
        disaster_type=DisasterType.EARTHQUAKE,
        sensor_value=magnitude,
        location="Test Location"
    )
    if should_alert:
        assert alert is not None
    else:
        assert alert is None
\end{lstlisting}

% ----------------------------------------------------------------------------
% EVALUATION RESULTS
% ----------------------------------------------------------------------------
\section{Evaluation Results}
\label{sec:results}

\subsection{Metrics Formula}

The evaluation framework calculates the following metrics:

\subsubsection{Test Success Rate}
\begin{equation}
    \text{Success Rate} = \frac{\text{Passed Tests}}{\text{Total Tests} - \text{Skipped Tests}} \times 100\%
\end{equation}

\subsubsection{Defect Density}
\begin{equation}
    \text{Defect Density} = \frac{\text{Failed Tests} + \text{Error Tests}}{\text{Total Tests}}
\end{equation}

\subsubsection{Code Coverage}
\begin{equation}
    \text{Coverage} = \frac{\text{Lines Executed}}{\text{Total Lines}} \times 100\%
\end{equation}

\subsection{Status Determination}

The system status is determined by the following criteria:

\textbf{PROD\_READY:}
\begin{align}
    &\text{Success Rate} \geq 95\% \\
    &\text{Code Coverage} \geq 80\% \\
    &\text{Defect Density} \leq 0.05
\end{align}

\textbf{STABLE:}
\begin{align}
    &\text{Success Rate} \geq 85\% \\
    &\text{Code Coverage} \geq 60\% \\
    &\text{Defect Density} \leq 0.15
\end{align}

\textbf{CRITICAL\_FAILURE:} Any metric below STABLE thresholds.

\subsection{Sample Evaluation Output}

\begin{verbatim}
============================================================
EVALUATION REPORT
============================================================
Generated: 2026-01-22T16:13:12

STATUS: [PASS] PROD_READY [PASS]

----------------------------------------
METRICS SUMMARY
----------------------------------------
  Total Tests:       79
  Passed:            79
  Failed:            0
  Errors:            0
  Skipped:           0
  Success Rate:      100.0%
  Code Coverage:     89.76%
  Defect Density:    0.0
  Execution Time:    1.749s

----------------------------------------
THRESHOLD REFERENCE
----------------------------------------
  PROD_READY:
    - Success Rate >= 95.0%
    - Code Coverage >= 80.0%
    - Defect Density <= 0.05
  STABLE:
    - Success Rate >= 85.0%
    - Code Coverage >= 60.0%
    - Defect Density <= 0.15

============================================================
\end{verbatim}

\subsection{Detailed Test Results Analysis}

The test execution on January 22, 2026 achieved a \textbf{100\% success rate} with all 79 tests passing. This section provides a thorough analysis of the results.

\subsubsection{Module Coverage Breakdown}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Module} & \textbf{Statements} & \textbf{Missed} & \textbf{Coverage} \\
\midrule
\texttt{src/\_\_init\_\_.py} & 2 & 0 & 100\% \\
\texttt{src/alerts/\_\_init\_\_.py} & 2 & 0 & 100\% \\
\texttt{src/alerts/alert\_manager.py} & 129 & 4 & 97\% \\
\texttt{src/api/\_\_init\_\_.py} & 2 & 0 & 100\% \\
\texttt{src/api/messaging.py} & 161 & 14 & 91\% \\
\texttt{src/core/\_\_init\_\_.py} & 3 & 0 & 100\% \\
\texttt{src/core/config.py} & 28 & 4 & 86\% \\
\texttt{src/core/logger.py} & 42 & 2 & 95\% \\
\texttt{src/db/\_\_init\_\_.py} & 2 & 0 & 100\% \\
\texttt{src/db/storage.py} & 166 & 31 & 81\% \\
\midrule
\textbf{TOTAL} & \textbf{537} & \textbf{55} & \textbf{89.76\%} \\
\bottomrule
\end{tabular}
\caption{Code Coverage by Module}
\end{table}

\subsubsection{Test Category Analysis}

\textbf{Functional Tests (18 tests, 100\% passed):}
\begin{itemize}
    \item \textbf{Alert Trigger Tests (FT-001 to FT-005):} Validate that alerts are correctly generated when sensor values exceed configured thresholds for earthquakes (5.0 magnitude), tsunamis (2.0m wave height), floods (3.0m water level), and cyclones (120 km/h wind speed).
    \item \textbf{Severity Classification Tests (FT-006 to FT-008):} Verify the five-level severity classification (LOW, MEDIUM, HIGH, CRITICAL, CATASTROPHIC) maps correctly to sensor values based on ISO 22324 guidelines.
    \item \textbf{Alert Acknowledgment Tests (FT-009, FT-010):} Confirm alerts can be marked as acknowledged and that attempting to acknowledge non-existent alerts returns graceful failures.
    \item \textbf{Notification Callback Tests (FT-011, FT-012):} Ensure registered notification callbacks are invoked when alerts are triggered, supporting multiple simultaneous callback registrations.
    \item \textbf{Statistics Tests (FT-013):} Validate accurate calculation of alert statistics including totals by type and severity.
\end{itemize}

\textbf{Boundary Value Analysis Tests (40 tests, 100\% passed):}
\begin{itemize}
    \item \textbf{Threshold Boundary Tests (BVA-001 to BVA-010):} Test exact boundary points for all disaster type thresholds. For earthquake threshold 5.0: values 4.99 (no alert), 5.0 (alert), 5.01 (alert). Similar parametrized tests for tsunami, flood, and cyclone thresholds.
    \item \textbf{Input Validation Tests (BVA-011 to BVA-013):} Verify graceful handling of invalid inputs including empty strings, whitespace-only locations, and extra spaces requiring trimming.
    \item \textbf{Phone Validation Tests (BVA-014):} Validate phone number formats with length boundaries (10-15 digits) and proper international format handling.
    \item \textbf{Email Validation Tests (BVA-015):} Test email format validation including minimum valid formats, missing components, and edge cases.
\end{itemize}

\textbf{Integration Tests (6 tests, 100\% passed):}
\begin{itemize}
    \item \textbf{End-to-End Flow (IT-001):} Complete sensor-to-SMS notification pipeline including AlertManager processing, callback invocation, and database storage.
    \item \textbf{Multi-Channel Notification (IT-002):} Simultaneous SMS and email delivery to multiple emergency contacts from database.
    \item \textbf{Database-Alert Linkage (IT-003):} Alert record persistence and location-based retrieval verification.
    \item \textbf{Acknowledgment Flow (IT-004):} Full acknowledgment lifecycle from alert creation through operator acknowledgment.
    \item \textbf{Region-Based Distribution (IT-005):} Filtering emergency contacts by geographic region for targeted alerts.
    \item \textbf{Priority Ordering (IT-006):} Verification that contacts are returned sorted by priority level.
\end{itemize}

\textbf{Stress Tests (6 tests, 100\% passed):}
\begin{itemize}
    \item \textbf{Burst Processing (ST-001):} 100 alerts processed in sequence in under 2 seconds (achieved 0.02s per alert).
    \item \textbf{Concurrent Processing (ST-002):} Multi-threaded alert processing with 5 threads handling 50 total alerts.
    \item \textbf{Notification Throughput (ST-003):} 100 notifications (50 SMS + 50 email) delivered within 5-second SLA.
    \item \textbf{Bulk Database Operations (ST-004, ST-005):} 100 contact insertions and concurrent query performance.
    \item \textbf{Extended Operation (ST-006):} Memory stability test with 200 alerts and periodic acknowledgment/cleanup.
\end{itemize}

\textbf{Safety/Risk-Based Tests (9 tests, 100\% passed):}
\begin{itemize}
    \item \textbf{Network Failure Scenarios (RBT-001 to RBT-003):} SMS and email gateway failures handled gracefully with SERVICE\_UNAVAILABLE status, retry logic exhaustion, and no system crashes.
    \item \textbf{Database Corruption (RBT-004, RBT-005):} Automatic fallback to file-based cache when primary database is corrupted, and graceful handling when no cache exists.
    \item \textbf{High Latency (RBT-006):} System continues functioning with 100ms simulated network latency per request.
    \item \textbf{Cascade Failure Prevention (RBT-007, RBT-008):} AlertManager continues recording alerts even when notification gateways are down; system recovers when connectivity is restored.
    \item \textbf{Data Integrity (RBT-009):} Alert records are preserved in history even when notification delivery fails.
\end{itemize}

\subsubsection{Key Findings and Observations}

\begin{enumerate}
    \item \textbf{Perfect Success Rate:} All 79 tests passed with zero failures, indicating robust implementation of core functionality.
    
    \item \textbf{High Coverage on Critical Modules:}
    \begin{itemize}
        \item Alert Manager: 97\% coverage --- the core alert processing logic is thoroughly tested
        \item Logger: 95\% coverage --- reliable logging infrastructure validated
        \item Messaging: 91\% coverage --- notification gateways well tested including failure modes
    \end{itemize}
    
    \item \textbf{Areas for Potential Coverage Improvement:}
    \begin{itemize}
        \item Storage module (81\%) --- some database edge cases could benefit from additional tests
        \item Config module (86\%) --- environment variable loading has some untested paths
    \end{itemize}
    
    \item \textbf{Fast Execution:} Total test execution time of 1.749 seconds enables rapid feedback during CI/CD pipelines.
    
    \item \textbf{Risk Mitigation Validated:} All CATASTROPHIC and CRITICAL risk scenarios (RISK-001 through RISK-005) were tested and passed, confirming the system handles failure modes appropriately.
\end{enumerate}

% ----------------------------------------------------------------------------
% CI/CD INTEGRATION
% ----------------------------------------------------------------------------
\section{CI/CD Integration}
\label{sec:cicd}

\subsection{Pipeline Architecture}

The GitHub Actions workflow implements a staged testing approach:

\begin{enumerate}
    \item \textbf{Functional Tests} - Core logic validation (must pass)
    \item \textbf{Integration Tests} - End-to-end flows (after functional)
    \item \textbf{Safety Tests} - Risk-based scenarios (parallel)
    \item \textbf{Stress Tests} - Performance (main branch only)
    \item \textbf{Evaluation} - Generate metrics report
    \item \textbf{Quality Gate} - Block deployment if CRITICAL\_FAILURE
\end{enumerate}

\subsection{Quality Gate}

The quality gate prevents deployment of code that fails to meet minimum standards:

\begin{lstlisting}[language=bash,caption={Quality Gate Check}]
STATUS=$(cat reports/evaluation.json | python -c \
    'import json,sys; print(json.load(sys.stdin)["status"])')

if [ "$STATUS" == "CRITICAL_FAILURE" ]; then
    echo "Quality Gate FAILED"
    exit 1
fi
\end{lstlisting}

% ----------------------------------------------------------------------------
% BIBLIOGRAPHY
% ----------------------------------------------------------------------------
\section{References and Bibliography}
\label{sec:references}

\subsection{Standards}

\begin{enumerate}
    \item IEEE 829-2008, ``IEEE Standard for Software and System Test Documentation''
    
    \item ISO/IEC/IEEE 29119-1:2022, ``Software and systems engineering --- Software testing --- Part 1: General concepts''
    
    \item ISO/IEC/IEEE 29119-2:2021, ``Software and systems engineering --- Software testing --- Part 2: Test processes''
    
    \item ISO/IEC/IEEE 29119-3:2021, ``Software and systems engineering --- Software testing --- Part 3: Test documentation''
    
    \item ISO/IEC/IEEE 29119-4:2021, ``Software and systems engineering --- Software testing --- Part 4: Test techniques''
    
    \item ISO 22324:2015, ``Societal security --- Emergency management --- Guidelines for colour-coded alerts''
    
    \item ISTQB Foundation Level Syllabus, Version 4.0, 2023
\end{enumerate}

\subsection{Justification for Methodology}

The testing methodology employed in this project follows industry best practices for mission-critical systems:

\begin{description}
    \item[Risk-Based Testing:] Essential for disaster alert systems where failure can result in loss of life. Prioritizes tests based on risk impact (ISO/IEC/IEEE 29119-4).
    
    \item[Boundary Value Analysis:] Critical for threshold-based systems. Ensures alerts trigger correctly at exact boundaries (ISTQB Foundation).
    
    \item[Failure Mode Simulation:] Validates system resilience during partial outages, which is common during actual disasters when infrastructure is compromised.
    
    \item[Automated Evaluation:] Provides objective, repeatable metrics for quality assessment, essential for CI/CD integration.
\end{description}

% ----------------------------------------------------------------------------
% CONCLUSION
% ----------------------------------------------------------------------------
\section{Conclusion}
\label{sec:conclusion}

\subsection{Summary of Achievements}

The Disaster Alert System (DAS) testing initiative has achieved all primary objectives:

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Objective} & \textbf{Target} & \textbf{Achieved} \\
\midrule
Total Test Cases & $\geq$ 50 & \textbf{79} \\
Test Success Rate & $\geq$ 95\% & \textbf{100.0\%} \\
Code Coverage & $\geq$ 80\% & \textbf{89.76\%} \\
Zero Critical Defects & 0 & \textbf{0} \\
Risk Mitigation Coverage & 100\% & \textbf{100\%} \\
\bottomrule
\end{tabular}
\caption{Achievement Summary}
\end{table}

\subsection{Key Accomplishments}

\begin{enumerate}
    \item \textbf{Comprehensive Test Coverage:} 79 test cases spanning five categories (Functional, Boundary, Integration, Stress, and Safety) ensure robustness across all system components.
    
    \item \textbf{Perfect Pass Rate:} All tests pass with zero failures, demonstrating high code quality and thorough implementation of requirements.
    
    \item \textbf{Mission-Critical Validation:} Risk-Based Testing validated all CATASTROPHIC and CRITICAL failure scenarios, confirming the system can operate safely during actual disaster events when infrastructure may be compromised.
    
    \item \textbf{Performance Verification:} Stress tests confirmed the system can handle burst loads of 100+ simultaneous alerts within acceptable time limits (under 2 seconds).
    
    \item \textbf{Standards Compliance:} Documentation follows IEEE 829, ISO/IEC/IEEE 29119, and ISTQB guidelines, providing industry-standard test artifacts.
\end{enumerate}

\subsection{System Status}

Based on the evaluation criteria:

\begin{center}
\fbox{\Large\textbf{STATUS: PROD\_READY}}
\end{center}

The system meets all thresholds for production deployment:
\begin{itemize}
    \item Success Rate: 100.0\% $\geq$ 95\% (threshold)
    \item Code Coverage: 89.76\% $\geq$ 80\% (threshold)
    \item Defect Density: 0.0 $\leq$ 0.05 (threshold)
\end{itemize}

\subsection{Recommendations}

\subsubsection{Short-Term (Before Production)}
\begin{itemize}
    \item Increase storage module coverage from 81\% to 90\% by adding tests for edge cases in cache operations
    \item Add performance benchmarks for geographic distance calculations used in alert radius filtering
\end{itemize}

\subsubsection{Long-Term (Post-Production)}
\begin{itemize}
    \item Implement end-to-end browser tests for the Frontend React application
    \item Add load testing with realistic production-scale data (1000+ contacts, 10000+ historical alerts)
    \item Establish baseline performance metrics for monitoring in production
    \item Consider chaos engineering tests for infrastructure failure scenarios
\end{itemize}

\subsection{Verification Statement}

This test documentation confirms that the Disaster Alert System has been verified and validated according to industry standards. The system is ready for production deployment with confidence that:

\begin{itemize}
    \item [\checkmark] Core alert functionality works correctly for all disaster types
    \item [\checkmark] Threshold-based triggering behaves predictably at exact boundary values
    \item [\checkmark] Notification delivery operates reliably across SMS and email channels
    \item [\checkmark] System degrades gracefully under failure conditions
    \item [\checkmark] Performance meets requirements under high-load conditions
\end{itemize}

% ----------------------------------------------------------------------------
% APPENDIX
% ----------------------------------------------------------------------------
\appendix

\section{Test Execution Commands}

\begin{lstlisting}[language=bash]
# Install dependencies
pip install -r requirements.txt

# Run all tests with coverage
pytest --cov=src --cov-report=html

# Run specific test categories
pytest tests/functional -v
pytest tests/safety -m safety
pytest tests/stress -m stress

# Run full evaluation
python tools/evaluator.py

# Generate reports only (no test execution)
python tools/evaluator.py --no-run
\end{lstlisting}

\section{Project Structure}

\begin{verbatim}
DAS_Project/
├── Backend/                        # Flask REST API Server
│   ├── app.py                      # Main Flask application
│   └── requirements.txt            # Python dependencies
│
├── Frontend/                       # React Web Application
│   ├── src/
│   │   ├── components/             # Reusable UI components
│   │   ├── pages/                  # Page components
│   │   ├── contexts/               # React contexts
│   │   ├── hooks/                  # Custom React hooks
│   │   ├── lib/                    # Utility libraries
│   │   └── test/                   # Frontend tests (Vitest)
│   ├── package.json                # Node.js dependencies
│   ├── vite.config.ts              # Vite configuration
│   ├── vitest.config.ts            # Vitest test config
│   └── tailwind.config.ts          # TailwindCSS config
│
├── src/                            # Core Python Library
│   ├── core/
│   │   ├── __init__.py
│   │   ├── config.py               # System configuration
│   │   └── logger.py               # Singleton logger
│   ├── alerts/
│   │   ├── __init__.py
│   │   └── alert_manager.py        # Alert processing logic
│   ├── api/
│   │   ├── __init__.py
│   │   └── messaging.py            # SMS/Email gateways
│   └── db/
│       ├── __init__.py
│       └── storage.py              # Database operations
│
├── tests/                          # Test Suite (79 tests)
│   ├── conftest.py                 # Pytest fixtures
│   ├── functional/                 # Core logic tests (18)
│   │   └── test_alerts.py
│   ├── integration/                # E2E flow tests (6)
│   │   └── test_flow.py
│   ├── boundary/                   # BVA tests (40)
│   │   └── test_limits.py
│   ├── stress/                     # Load tests (6)
│   │   └── test_load.py
│   ├── safety/                     # Risk-based tests (9)
│   │   └── test_failures.py
│   ├── tools/
│   │   └── evaluator.py            # Test evaluation framework
│   ├── docs/
│   │   └── latex/
│   │       └── das_report.tex      # This document
│   └── reports/                    # Generated reports
│       ├── junit_report.xml
│       ├── coverage.json
│       └── evaluation_report.json
│
├── reports/                        # Coverage reports
├── cache/                          # Fallback cache files
├── .github/workflows/              # CI/CD pipelines
│   └── main.yml
└── README.md                       # Project documentation
\end{verbatim}

\end{document}
